#!/usr/bin/env python3
"""
üöÄ –£–õ–£–ß–®–ï–ù–ù–´–ô –¢–û–†–ì–û–í–´–ô –ë–û–¢ v5.0
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥ –∏ –æ–±—É—á–µ–Ω–∏—è
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import ccxt
import time
import os
import signal
from datetime import datetime
from typing import Dict, Tuple, Optional, List
import json
from dataclasses import dataclass

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è RL
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import BaseCallback
import torch
import torch.nn as nn

from config import Config


@dataclass
class Trade:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å–¥–µ–ª–∫–∏"""
    entry_price: float
    entry_time: int
    position_size: float
    trade_type: str
    stop_loss: float = 0.0
    take_profit: float = 0.0
    steps_held: int = 0


class TradeTracker:
    """–¢—Ä–µ–∫–µ—Ä –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç–æ—Ä–≥–æ–≤—ã—Ö –ø–æ–∑–∏—Ü–∏–π"""
    
    def __init__(self):
        self.active_trades: List[Trade] = []
        self.closed_trades: List[Dict] = []
        self.total_commission_paid = 0.0
        
    def open_trade(self, entry_price: float, position_size: float, 
                   trade_type: str, stop_loss: float = 0.0, take_profit: float = 0.0) -> Trade:
        trade = Trade(
            entry_price=entry_price,
            entry_time=int(time.time()),
            position_size=position_size,
            trade_type=trade_type,
            stop_loss=stop_loss,
            take_profit=take_profit,
            steps_held=0
        )
        self.active_trades.append(trade)
        return trade
    
    def close_trade(self, exit_price: float, commission: float = 0.0) -> Dict:
        if not self.active_trades:
            return {}
            
        trade = self.active_trades.pop(0)
        
        if trade.trade_type == 'LONG':
            pnl = trade.position_size * (exit_price - trade.entry_price)
        else:
            pnl = trade.position_size * (trade.entry_price - exit_price)
            
        pnl_percent = (pnl / (trade.position_size * trade.entry_price)) * 100
        pnl_after_commission = pnl - commission
        
        self.total_commission_paid += commission
        
        result = {
            'pnl': pnl,
            'pnl_percent': pnl_percent,
            'pnl_after_commission': pnl_after_commission,
            'duration': int(time.time()) - trade.entry_time,
            'steps_held': trade.steps_held
        }
        
        self.closed_trades.append(result)
        return result
    
    def get_unrealized_pnl(self, current_price: float) -> float:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–∏–π –Ω–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π PnL"""
        total_unrealized = 0.0
        for trade in self.active_trades:
            if trade.trade_type == 'LONG':
                unrealized = trade.position_size * (current_price - trade.entry_price)
            else:
                unrealized = trade.position_size * (trade.entry_price - current_price)
            total_unrealized += unrealized
        return total_unrealized
    
    def update_steps(self):
        """–£–≤–µ–ª–∏—á–∏—Ç—å —Å—á–µ—Ç—á–∏–∫ —à–∞–≥–æ–≤ –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–¥–µ–ª–æ–∫"""
        for trade in self.active_trades:
            trade.steps_held += 1


class ImprovedTradingEnvironment(gym.Env):
    """–£–ª—É—á—à–µ–Ω–Ω–æ–µ —Ç–æ—Ä–≥–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –Ω–∞–≥—Ä–∞–¥"""
    
    def __init__(self, initial_balance: float = 10000, detailed_logging: bool = True):
        super().__init__()
        
        self.initial_balance = initial_balance
        self.detailed_logging = detailed_logging
        
        # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è SAC
        # action[0]: —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏ (-1 = –ø—Ä–æ–¥–∞—Ç—å –≤—Å–µ, 0 = –¥–µ—Ä–∂–∞—Ç—å, 1 = –∫—É–ø–∏—Ç—å –º–∞–∫—Å–∏–º—É–º)
        self.action_space = spaces.Box(
            low=np.array([-1.0], dtype=np.float32),
            high=np.array([1.0], dtype=np.float32),
            dtype=np.float32
        )
        
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π (40 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(40,), dtype=np.float32
        )
        
        self._init_exchange()
        self._init_state()
        
    def _init_exchange(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Binance"""
        try:
            self.exchange = ccxt.binance({
                'apiKey': Config.API_KEY,
                'secret': Config.API_SECRET,
                'enableRateLimit': True,
                'options': {'defaultType': 'spot'}
            })
            self.exchange.set_sandbox_mode(True)
            print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω –∫ Binance Testnet")
        except:
            print("üìä –†–∞–±–æ—Ç–∞–µ–º –≤ —Ä–µ–∂–∏–º–µ —Å–∏–º—É–ª—è—Ü–∏–∏")
            self.exchange = None
            
    def _init_state(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        self.balance = self.initial_balance
        self.btc_amount = 0.0
        self.current_price = 67000.0
        
        # –ò—Å—Ç–æ—Ä–∏—è
        self.price_history = []
        self.action_history = []
        self.reward_history = []
        
        # –¢—Ä–µ–∫–µ—Ä —Å–¥–µ–ª–æ–∫
        self.trade_tracker = TradeTracker()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_steps = 0
        self.consecutive_holds = 0
        self.last_action_type = 'HOLD'
        
    def reset(self, seed: Optional[int] = None) -> Tuple[np.ndarray, Dict]:
        """–°–±—Ä–æ—Å –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        super().reset(seed=seed)
        self._init_state()
        self._update_market_data()
        return self._get_observation(), {}
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è"""
        self.total_steps += 1

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ü–ï–†–ï–î —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self._update_market_data()

        # –ó–∞—â–∏—Ç–∞ –æ—Ç None —Ü–µ–Ω—ã
        if self.current_price is None or self.current_price <= 0:
            print(f"‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: current_price = {self.current_price}, —Å–±—Ä–æ—Å –Ω–∞ 67000")
            self.current_price = 67000.0

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        prev_portfolio_value = self._get_portfolio_value()
        prev_balance = self.balance
        prev_btc = self.btc_amount
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ —à–∞–≥–æ–≤ –¥–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–¥–µ–ª–æ–∫
        self.trade_tracker.update_steps()
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ
        action_result = self._execute_action(action[0])  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –º–∞—Å—Å–∏–≤–∞
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–≥—Ä–∞–¥—É
        reward = self._calculate_reward(
            action[0],
            action_result,
            prev_portfolio_value,
            prev_balance,
            prev_btc
        )
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        self.action_history.append(action[0])
        self.reward_history.append(reward)
        if len(self.action_history) > 100:
            self.action_history = self.action_history[-100:]
            self.reward_history = self.reward_history[-100:]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        current_portfolio_value = self._get_portfolio_value()
        terminated = False
        truncated = current_portfolio_value < self.initial_balance * 0.1  # –ü–æ—Ç–µ—Ä—è 90%
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        info = {
            'portfolio_value': current_portfolio_value,
            'balance': self.balance,
            'btc_amount': self.btc_amount,
            'current_price': self.current_price,
            'action_type': action_result['type'],
            'action_success': action_result['success'],
            'unrealized_pnl': self.trade_tracker.get_unrealized_pnl(self.current_price),
            'active_trades': len(self.trade_tracker.active_trades),
            'closed_trades': len(self.trade_tracker.closed_trades)
        }
        
        return self._get_observation(), reward, terminated, truncated, info
    
    def _execute_action(self, action: float) -> Dict:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏–µ–º"""
        result = {'type': 'UNKNOWN', 'success': False, 'details': {}}

        # –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω—ã
        if self.current_price is None or self.current_price <= 0:
            print(f"‚ö†Ô∏è –û–®–ò–ë–ö–ê –≤ _execute_action: current_price = {self.current_price}")
            self.current_price = 67000.0

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        # -1.0 –¥–æ -0.1: SELL (–ø—Ä–æ–¥–∞–∂–∞)
        # -0.1 –¥–æ +0.1: HOLD (—É–¥–µ—Ä–∂–∞–Ω–∏–µ)
        # +0.1 –¥–æ +1.0: BUY (–ø–æ–∫—É–ø–∫–∞)
        
        # HOLD
        if -0.1 <= action <= 0.1:
            self.consecutive_holds += 1
            result['type'] = 'HOLD'
            result['success'] = True
            
            if self.detailed_logging and self.total_steps % 10 == 0:
                unrealized = self.trade_tracker.get_unrealized_pnl(self.current_price)
                print(f"‚ö™ HOLD @ ${self.current_price:.2f} | Unrealized: ${unrealized:+.2f}")
                
        # BUY
        elif action > 0.1:
            self.consecutive_holds = 0
            
            # –†–∞–∑–º–µ—Ä –ø–æ–∫—É–ø–∫–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞
            buy_strength = min(1.0, max(0.1, abs(action)))
            max_buy = self.balance * 0.4 * buy_strength  # –î–æ 40% –±–∞–ª–∞–Ω—Å–∞
            
            if max_buy < 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Å–¥–µ–ª–∫–∞ $100
                result['type'] = 'BUY_FAILED'
                result['success'] = False
                result['details']['reason'] = 'insufficient_funds'
                
                if self.detailed_logging:
                    print(f"‚õî BUY FAILED: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å—Ä–µ–¥—Å—Ç–≤ (${self.balance:.2f})")
            else:
                # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∫—É–ø–∫—É
                btc_to_buy = max_buy / self.current_price
                commission = max_buy * 0.001
                
                if max_buy + commission > self.balance:
                    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–∫—É–ø–∫–∏
                    max_buy = self.balance * 0.95  # –û—Å—Ç–∞–≤–ª—è–µ–º 5% –Ω–∞ –∫–æ–º–∏—Å—Å–∏—é
                    btc_to_buy = max_buy / self.current_price
                    commission = max_buy * 0.001
                
                self.balance -= (max_buy + commission)
                self.btc_amount += btc_to_buy
                
                # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Å–¥–µ–ª–∫—É
                self.trade_tracker.open_trade(
                    entry_price=self.current_price,
                    position_size=btc_to_buy,
                    trade_type='LONG'
                )
                
                result['type'] = 'BUY'
                result['success'] = True
                result['details'] = {
                    'amount': btc_to_buy,
                    'price': self.current_price,
                    'cost': max_buy,
                    'strength': buy_strength
                }
                
                print(f"üü¢ BUY: {btc_to_buy:.6f} BTC @ ${self.current_price:.2f} | Cost: ${max_buy:.2f} | Strength: {buy_strength:.2f}")
                
        # SELL
        elif action < -0.1:
            self.consecutive_holds = 0
            
            if self.btc_amount < 0.0001:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –ø—Ä–æ–¥–∞–∂–∏
                result['type'] = 'SELL_FAILED'
                result['success'] = False
                result['details']['reason'] = 'no_position'
                
                if self.detailed_logging:
                    print(f"‚õî SELL FAILED: –ù–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–¥–∞–∂–∏")
            else:
                # –†–∞–∑–º–µ—Ä –ø—Ä–æ–¥–∞–∂–∏ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–∏–ª—ã —Å–∏–≥–Ω–∞–ª–∞
                sell_strength = min(1.0, max(0.1, abs(action)))
                btc_to_sell = min(self.btc_amount, self.btc_amount * sell_strength)
                
                sell_amount = btc_to_sell * self.current_price
                commission = sell_amount * 0.001
                
                # –ò—Å–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–¥–∞–∂—É
                self.balance += (sell_amount - commission)
                self.btc_amount -= btc_to_sell
                
                # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–¥–µ–ª–∫–∏
                closed_trades = []
                trades_to_close = max(1, int(len(self.trade_tracker.active_trades) * sell_strength))
                
                for _ in range(min(trades_to_close, len(self.trade_tracker.active_trades))):
                    trade_result = self.trade_tracker.close_trade(
                        exit_price=self.current_price,
                        commission=commission / max(1, trades_to_close)
                    )
                    if trade_result:
                        closed_trades.append(trade_result)
                
                result['type'] = 'SELL'
                result['success'] = True
                result['details'] = {
                    'amount': btc_to_sell,
                    'price': self.current_price,
                    'revenue': sell_amount,
                    'closed_trades': closed_trades,
                    'strength': sell_strength
                }
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                if closed_trades:
                    total_pnl = sum(t['pnl_after_commission'] for t in closed_trades)
                    avg_pnl_pct = np.mean([t['pnl_percent'] for t in closed_trades])
                    print(f"üî¥ SELL: {btc_to_sell:.6f} BTC @ ${self.current_price:.2f} | Strength: {sell_strength:.2f}")
                    print(f"   üìä Closed {len(closed_trades)} trades | Total PnL: ${total_pnl:+.2f} ({avg_pnl_pct:+.2f}%)")
                else:
                    print(f"üî¥ SELL: {btc_to_sell:.6f} BTC @ ${self.current_price:.2f} | Strength: {sell_strength:.2f}")
        
        self.last_action_type = result['type']
        return result
    
    def _calculate_reward(self, action: float, action_result: Dict,
                         prev_portfolio: float, prev_balance: float,
                         prev_btc: float) -> float:
        """
        –£–õ–£–ß–®–ï–ù–ù–ê–Ø –°–ë–ê–õ–ê–ù–°–ò–†–û–í–ê–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ù–ê–ì–†–ê–î
        –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∫ –º–∞—Å—à—Ç–∞–±—É [-10, +10]
        """
        reward = 0.0

        # 1. –û–°–ù–û–í–ù–ê–Ø –ù–ê–ì–†–ê–î–ê: –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö
        current_portfolio = self._get_portfolio_value()
        portfolio_change_pct = ((current_portfolio - prev_portfolio) / prev_portfolio) * 100
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º: 1% –ø—Ä–∏–±—ã–ª–∏ = +10 –Ω–∞–≥—Ä–∞–¥—ã
        reward += np.clip(portfolio_change_pct * 10, -20, 20)

        # 2. –ù–ê–ì–†–ê–î–ê –ó–ê –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ó–ê–ö–†–´–¢–´–• –°–î–ï–õ–û–ö
        if action_result['type'] == 'SELL' and action_result['success']:
            closed_trades = action_result['details'].get('closed_trades', [])

            for trade in closed_trades:
                pnl_pct = trade['pnl_percent']

                # –ù–µ–ª–∏–Ω–µ–π–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: –±–æ–ª—å—à–µ –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –±–æ–ª—å—à—É—é –ø—Ä–∏–±—ã–ª—å
                if pnl_pct > 0:
                    # –ü—Ä–∏–±—ã–ª—å–Ω–∞—è —Å–¥–µ–ª–∫–∞: –æ—Ç 0 –¥–æ +15
                    trade_reward = np.clip(pnl_pct * 3, 0, 15)
                else:
                    # –£–±—ã—Ç–æ—á–Ω–∞—è —Å–¥–µ–ª–∫–∞: –æ—Ç 0 –¥–æ -15
                    trade_reward = np.clip(pnl_pct * 3, -15, 0)

                reward += trade_reward

                # –ë–æ–Ω—É—Å –∑–∞ —Ç–µ—Ä–ø–µ–Ω–∏–µ —Å –ø—Ä–∏–±—ã–ª—å–Ω–æ–π –ø–æ–∑–∏—Ü–∏–µ–π
                if pnl_pct > 0.5 and trade['steps_held'] > 10:
                    patience_bonus = min(5, trade['steps_held'] * 0.2)
                    reward += patience_bonus

                # –®—Ç—Ä–∞—Ñ –∑–∞ –±—ã—Å—Ç—Ä—É—é –ø—Ä–æ–¥–∞–∂—É –≤ —É–±—ã—Ç–æ–∫
                if pnl_pct < -1 and trade['steps_held'] < 5:
                    reward -= 3

        # 3. –ù–ê–ì–†–ê–î–ê –ó–ê –£–î–ï–†–ñ–ê–ù–ò–ï –ü–†–ò–ë–´–õ–¨–ù–´–• –ü–û–ó–ò–¶–ò–ô (HOLD)
        if abs(action) <= 0.1 and self.trade_tracker.active_trades:
            unrealized_pnl = self.trade_tracker.get_unrealized_pnl(self.current_price)
            unrealized_pct = (unrealized_pnl / prev_portfolio) * 100

            if unrealized_pct > 1:  # –ü—Ä–∏–±—ã–ª—å > 1%
                reward += min(5, unrealized_pct)  # –ü–æ–æ—â—Ä—è–µ–º —Ç–µ—Ä–ø–µ–Ω–∏–µ
            elif unrealized_pct < -2:  # –£–±—ã—Ç–æ–∫ > 2%
                reward -= min(5, abs(unrealized_pct) * 0.5)  # –ù–∞–∫–∞–∑—ã–≤–∞–µ–º —É–¥–µ—Ä–∂–∞–Ω–∏–µ —É–±—ã—Ç–∫–æ–≤

        # 4. –®–¢–†–ê–§–´ –ó–ê –ù–ï–£–î–ê–ß–ù–´–ï –î–ï–ô–°–¢–í–ò–Ø
        if not action_result['success']:
            if abs(action) > 0.3:  # –°–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª, –Ω–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª
                reward -= 3
            elif abs(action) > 0.1:  # –°–ª–∞–±—ã–π —Å–∏–≥–Ω–∞–ª
                reward -= 1

        # 5. –ü–û–û–©–†–ï–ù–ò–ï –ê–ö–¢–ò–í–ù–û–°–¢–ò –Ω–∞ —Å—Ç–∞—Ä—Ç–µ
        if self.total_steps < 1000 and action_result['success'] and abs(action) > 0.1:
            reward += 1

        # 6. –®–¢–†–ê–§ –ó–ê –ß–†–ï–ó–ú–ï–†–ù–û–ï –ë–ï–ó–î–ï–ô–°–¢–í–ò–ï
        if self.consecutive_holds > 30 and not self.trade_tracker.active_trades:
            reward -= 2

        # Clip —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä–∞–¥—ã –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        reward = np.clip(reward, -50, 50)

        return reward
    
    def _update_market_data(self):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫"""
        try:
            if self.exchange:
                ticker = self.exchange.fetch_ticker('BTC/USDT')
                new_price = ticker.get('last', None)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω–æ–π —Ü–µ–Ω—ã
                if new_price and isinstance(new_price, (int, float)) and new_price > 0:
                    self.current_price = float(new_price)
                else:
                    # –ï—Å–ª–∏ —Ü–µ–Ω–∞ –Ω–µ–≤–∞–ª–∏–¥–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –∏–∑–≤–µ—Å—Ç–Ω—É—é
                    if self.current_price is None or self.current_price <= 0:
                        self.current_price = 67000.0
                    # –∏–Ω–∞—á–µ –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â—É—é —Ü–µ–Ω—É
            else:
                # –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–∞—è —Å–∏–º—É–ª—è—Ü–∏—è —Ü–µ–Ω—ã
                if self.current_price is None or len(self.price_history) == 0:
                    self.current_price = 67000.0
                else:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–µ–Ω–¥ –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
                    trend = np.random.choice([-0.0001, 0, 0.0001], p=[0.3, 0.4, 0.3])
                    volatility = np.random.normal(0, 0.002)
                    new_price = self.current_price * (1 + trend + volatility)
                    self.current_price = max(50000, min(80000, new_price))

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Ü–µ–Ω—ã
            if self.current_price and self.current_price > 0:
                self.price_history.append(self.current_price)
                if len(self.price_history) > 100:
                    self.price_history = self.price_history[-100:]

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            if self.current_price is None or self.current_price <= 0:
                self.current_price = 67000.0
                if len(self.price_history) == 0:
                    self.price_history.append(self.current_price)
    
    def _get_portfolio_value(self) -> float:
        """–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫"""
        if self.current_price is None or self.current_price <= 0:
            # –ê–≤–∞—Ä–∏–π–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
            self.current_price = 67000.0
        return self.balance + (self.btc_amount * self.current_price)
    
    def _get_observation(self) -> np.ndarray:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
        obs = []
        
        # 1. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ü–µ–Ω–∞ –∏ —Ç—Ä–µ–Ω–¥ (10 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        obs.append(self.current_price / 70000)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–∫–æ–ª–æ —Å—Ä–µ–¥–Ω–µ–≥–æ
        
        # –¶–µ–Ω–æ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞—Ö
        for lookback in [1, 2, 5, 10, 20]:
            if len(self.price_history) > lookback:
                price_change = (self.current_price - self.price_history[-lookback]) / self.price_history[-lookback]
                obs.append(price_change * 100)
            else:
                obs.append(0)
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
        if len(self.price_history) >= 20:
            sma_5 = np.mean(self.price_history[-5:])
            sma_20 = np.mean(self.price_history[-20:])
            obs.append((self.current_price - sma_5) / sma_5)
            obs.append((self.current_price - sma_20) / sma_20)
            obs.append((sma_5 - sma_20) / sma_20)
        else:
            obs.extend([0, 0, 0])
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        if len(self.price_history) >= 10:
            returns = np.diff(self.price_history[-10:]) / self.price_history[-10:-1]
            obs.append(np.std(returns) * 100)
        else:
            obs.append(0)
        
        # 2. –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è (10 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        portfolio_value = self._get_portfolio_value()
        
        obs.extend([
            self.balance / self.initial_balance,
            self.btc_amount * self.current_price / self.initial_balance if self.current_price > 0 else 0,
            (portfolio_value - self.initial_balance) / self.initial_balance,  # ROI
            len(self.trade_tracker.active_trades) / 10,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–¥–µ–ª–æ–∫
        ])
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –Ω–µ—Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–º PnL
        if self.trade_tracker.active_trades:
            unrealized = self.trade_tracker.get_unrealized_pnl(self.current_price)
            unrealized_pct = unrealized / self.initial_balance
            avg_entry = np.mean([t.entry_price for t in self.trade_tracker.active_trades])
            avg_hold_time = np.mean([t.steps_held for t in self.trade_tracker.active_trades])
            
            obs.extend([
                unrealized_pct * 10,  # –£—Å–∏–ª–∏–≤–∞–µ–º —Å–∏–≥–Ω–∞–ª
                (self.current_price - avg_entry) / avg_entry,
                avg_hold_time / 100,
            ])
        else:
            obs.extend([0, 0, 0])
        
        # –î–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –¥–ª—è –ø–æ–∫—É–ø–∫–∏ (–≤–∞–∂–Ω–æ!)
        max_buy_amount = self.balance * 0.3
        can_buy = 1.0 if max_buy_amount >= 100 else 0.0
        can_sell = 1.0 if self.btc_amount >= 0.0001 else 0.0
        
        obs.extend([can_buy, can_sell, max_buy_amount / self.initial_balance])
        
        # 3. –ò—Å—Ç–æ—Ä–∏—è —Ç–æ—Ä–≥–æ–≤–ª–∏ (10 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        if self.trade_tracker.closed_trades:
            recent_trades = self.trade_tracker.closed_trades[-10:]
            
            wins = sum(1 for t in recent_trades if t['pnl_after_commission'] > 0)
            win_rate = wins / len(recent_trades)
            
            avg_win = np.mean([t['pnl_percent'] for t in recent_trades if t['pnl_after_commission'] > 0]) if wins > 0 else 0
            avg_loss = np.mean([t['pnl_percent'] for t in recent_trades if t['pnl_after_commission'] <= 0]) if wins < len(recent_trades) else 0
            
            total_pnl = sum(t['pnl_after_commission'] for t in recent_trades)
            
            obs.extend([
                win_rate,
                avg_win / 100,
                avg_loss / 100,
                total_pnl / self.initial_balance,
                len(self.trade_tracker.closed_trades) / 100,
            ])
        else:
            obs.extend([0, 0, 0, 0, 0])
        
        # –ò—Å—Ç–æ—Ä–∏—è –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –¥–µ–π—Å—Ç–≤–∏–π
        if len(self.action_history) >= 10:
            recent_actions = self.action_history[-10:]
            action_distribution = [
                recent_actions.count(0) / 10,  # % HOLD
                recent_actions.count(1) / 10,  # % BUY
                recent_actions.count(2) / 10,  # % SELL
            ]
            obs.extend(action_distribution)
        else:
            obs.extend([0, 0, 0])
        
        # –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è
        if len(self.reward_history) >= 10:
            avg_recent_reward = np.mean(self.reward_history[-10:])
            obs.append(avg_recent_reward / 100)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
        else:
            obs.append(0)
        
        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —É—Å–ø–µ—à–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
        obs.append(1.0 if self.last_action_type in ['BUY', 'SELL', 'HOLD'] else -1.0)
        
        # 4. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (10 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        obs.extend([
            self.consecutive_holds / 20,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ HOLD –ø–æ–¥—Ä—è–¥
            self.total_steps / 1000,  # –ü—Ä–æ–≥—Ä–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
            np.sin(self.total_steps * 0.01),  # –í—Ä–µ–º–µ–Ω–Ω–æ–π —Å–∏–≥–Ω–∞–ª –¥–ª—è —Ü–∏–∫–ª–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            np.cos(self.total_steps * 0.01),
        ])
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º –¥–æ 40 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        while len(obs) < 40:
            obs.append(0.0)
        
        return np.array(obs[:40], dtype=np.float32)


class CustomCallback(BaseCallback):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π callback –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –æ–±—É—á–µ–Ω–∏—è"""

    def __init__(self, verbose=0, log_freq=500):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_lengths = []
        self.log_freq = log_freq
        self.best_mean_reward = -np.inf
        self.episode_count = 0

    def _on_step(self) -> bool:
        # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
        if self.n_calls % self.log_freq == 0 and len(self.episode_rewards) > 0:
            recent_rewards = self.episode_rewards[-20:] if len(self.episode_rewards) >= 20 else self.episode_rewards
            mean_reward = np.mean(recent_rewards)
            std_reward = np.std(recent_rewards)

            # –ü–æ–ª—É—á–∞–µ–º info –∏–∑ env –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ
            if 'infos' in self.locals and len(self.locals['infos']) > 0:
                info = self.locals['infos'][0]
                portfolio = info.get('portfolio_value', 0)
                active_trades = info.get('active_trades', 0)
                closed_trades = info.get('closed_trades', 0)

                print(f"\n{'='*60}")
                print(f"üìä –ü–†–û–ì–†–ï–°–° –û–ë–£–ß–ï–ù–ò–Ø - –®–∞–≥ {self.n_calls}")
                print(f"{'='*60}")
                print(f"üèÜ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (20 —ç–ø–∏–∑–æ–¥–æ–≤): {mean_reward:.2f} ¬± {std_reward:.2f}")
                print(f"üí∞ –ü–æ—Ä—Ç—Ñ–µ–ª—å: ${portfolio:.2f}")
                print(f"üìà –ê–∫—Ç–∏–≤–Ω—ã—Ö —Å–¥–µ–ª–æ–∫: {active_trades} | –ó–∞–∫—Ä—ã—Ç—ã—Ö: {closed_trades}")
                print(f"üéØ –õ—É—á—à–∞—è —Å—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {self.best_mean_reward:.2f}")
                print(f"{'='*60}\n")

                # –û–±–Ω–æ–≤–ª—è–µ–º –ª—É—á—à—É—é –Ω–∞–≥—Ä–∞–¥—É
                if mean_reward > self.best_mean_reward:
                    self.best_mean_reward = mean_reward
                    print(f"üåü –ù–û–í–´–ô –†–ï–ö–û–†–î! –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞: {mean_reward:.2f}\n")

        return True

    def _on_rollout_end(self) -> None:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —ç–ø–∏–∑–æ–¥–∞
        if 'infos' in self.locals and len(self.locals['infos']) > 0:
            for info in self.locals['infos']:
                if 'episode' in info:
                    ep_rew = info['episode']['r']
                    ep_len = info['episode']['l']
                    self.episode_rewards.append(ep_rew)
                    self.episode_lengths.append(ep_len)
                    self.episode_count += 1


class ImprovedTradingAgent:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –∞–≥–µ–Ω—Ç —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""

    def __init__(self, env: ImprovedTradingEnvironment, model_path: Optional[str] = None):
        self.env = env

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º device (GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"üñ•Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è: {self.device}")
        if torch.cuda.is_available():
            print(f"   GPU: {torch.cuda.get_device_name(0)}")

        # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–ª–∏—Ç–∏–∫–∏
        policy_kwargs = dict(
            activation_fn=nn.Tanh,  # Tanh –ª—É—á—à–µ –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
            net_arch=dict(
                pi=[256, 128],  # –ú–µ–Ω—å—à–µ —Å–ª–æ–µ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                qf=[256, 128]
            ),
            optimizer_kwargs=dict(
                eps=1e-5  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            )
        )

        if model_path and os.path.exists(model_path):
            self.model = SAC.load(model_path, env=env, device=self.device)
            print(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {model_path}")
        else:
            self.model = SAC(
                "MlpPolicy",
                env,
                learning_rate=1e-4,  # –ú–µ–Ω—å—à–µ learning rate –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                buffer_size=100000,  # –ë–æ–ª—å—à–∏–π buffer –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                learning_starts=1000,  # –ë–æ–ª—å—à–µ initial exploration
                batch_size=256,      # –ë–æ–ª—å—à–∏–π batch –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
                tau=0.005,          # –ú–µ–¥–ª–µ–Ω–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ target network
                gamma=0.99,         # –í—ã—à–µ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –Ω–∞–≥—Ä–∞–¥
                train_freq=1,
                gradient_steps=1,
                ent_coef='auto',    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —ç–Ω—Ç—Ä–æ–ø–∏–∏
                target_update_interval=1,
                target_entropy='auto',
                use_sde=False,
                policy_kwargs=policy_kwargs,
                device=self.device,  # ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU
                verbose=1
            )
            print("üÜï –°–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π —É–ª—É—á—à–µ–Ω–Ω—ã–π SAC –∞–≥–µ–Ω—Ç")
        
        self.callback = CustomCallback()
    
    def train(self, total_timesteps: int = 10000, save_best: bool = True):
        """–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞ —Å –∞–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏"""
        print(f"\nüéì –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {total_timesteps} —à–∞–≥–æ–≤...")
        print(f"üíæ –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏: {'–í–∫–ª' if save_best else '–í—ã–∫–ª'}\n")

        try:
            self.model.learn(
                total_timesteps=total_timesteps,
                callback=self.callback,
                progress_bar=True,
                reset_num_timesteps=False  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å —Ç–µ–∫—É—â–µ–≥–æ —à–∞–≥–∞
            )

            print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            self._print_training_stats()

            # –ê–≤—Ç–æ—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
            if save_best:
                self.save("./models/improved_bot_latest")
                if self.callback.best_mean_reward > -np.inf:
                    print(f"üíæ –ú–æ–¥–µ–ª—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

        except KeyboardInterrupt:
            print("\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            if save_best:
                self.save("./models/improved_bot_interrupted")
                print("üíæ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω")

    def _print_training_stats(self):
        """–í—ã–≤–æ–¥ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è"""
        if self.callback.episode_rewards:
            rewards = self.callback.episode_rewards
            recent_100 = rewards[-100:] if len(rewards) >= 100 else rewards
            recent_20 = rewards[-20:] if len(rewards) >= 20 else rewards

            print(f"""
{'='*60}
üìà –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–£–ß–ï–ù–ò–Ø
{'='*60}
üìä –í—Å–µ–≥–æ —ç–ø–∏–∑–æ–¥–æ–≤: {len(rewards)}
üèÜ –õ—É—á—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {max(rewards):.2f}
üíî –•—É–¥—à–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {min(rewards):.2f}
üìà –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100): {np.mean(recent_100):.2f} ¬± {np.std(recent_100):.2f}
üéØ –°—Ä–µ–¥–Ω—è—è –Ω–∞–≥—Ä–∞–¥–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 20): {np.mean(recent_20):.2f} ¬± {np.std(recent_20):.2f}
üåü –†–µ–∫–æ—Ä–¥ —Å—Ä–µ–¥–Ω–µ–π –Ω–∞–≥—Ä–∞–¥—ã: {self.callback.best_mean_reward:.2f}
{'='*60}
            """)
    
    def save(self, path: str = "./models/improved_bot"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
        os.makedirs(os.path.dirname(path), exist_ok=True)
        self.model.save(path)
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {path}")
    
    def test(self, episodes: int = 5):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞"""
        print(f"\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ {episodes} —ç–ø–∏–∑–æ–¥–∞—Ö...")
        
        for episode in range(episodes):
            obs, _ = self.env.reset()
            done = False
            truncated = False
            total_reward = 0
            steps = 0
            
            print(f"\nüìç –≠–ø–∏–∑–æ–¥ {episode + 1}")
            
            while not done and not truncated and steps < 200:
                action, _ = self.model.predict(obs, deterministic=True)
                obs, reward, done, truncated, info = self.env.step(action)
                total_reward += reward
                steps += 1
                
                if steps % 20 == 0:
                    print(f"  –®–∞–≥ {steps}: Portfolio=${info['portfolio_value']:.2f}, Reward={reward:+.2f}")
            
            final_value = self.env._get_portfolio_value()
            roi = (final_value - self.env.initial_balance) / self.env.initial_balance * 100
            
            print(f"""
üìä –†–µ–∑—É–ª—å—Ç–∞—Ç —ç–ø–∏–∑–æ–¥–∞ {episode + 1}:
  üí∞ –§–∏–Ω–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å: ${final_value:.2f}
  üìà ROI: {roi:+.2f}%
  üèÜ –û–±—â–∞—è –Ω–∞–≥—Ä–∞–¥–∞: {total_reward:.2f}
  üìç –®–∞–≥–æ–≤: {steps}
  üìä –ó–∞–∫—Ä—ã—Ç–æ —Å–¥–µ–ª–æ–∫: {len(self.env.trade_tracker.closed_trades)}
            """)


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("""
üöÄ –£–õ–£–ß–®–ï–ù–ù–´–ô –¢–û–†–ì–û–í–´–ô –ë–û–¢ v6.0
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
‚úÖ GPU Acceleration (CUDA)
‚úÖ –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥
‚úÖ –£–ª—É—á—à–µ–Ω–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π
‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –æ–±—É—á–µ–Ω–∏—è
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
    """)

    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    env = ImprovedTradingEnvironment(initial_balance=10000, detailed_logging=False)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
    model_path = "./models/improved_bot_latest.zip"
    if os.path.exists(model_path):
        print(f"üìÇ –ù–∞–π–¥–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {model_path}")
        response = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ —Å —ç—Ç–æ–π –º–æ–¥–µ–ª–∏? (y/n): ").strip().lower()
        if response == 'y':
            agent = ImprovedTradingAgent(env, model_path=model_path)
            print("‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n")
        else:
            agent = ImprovedTradingAgent(env)
            print("‚úÖ –°–æ–∑–¥–∞–Ω–∞ –Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å\n")
    else:
        agent = ImprovedTradingAgent(env)

    # –û–±—É—á–µ–Ω–∏–µ
    print("\n1Ô∏è‚É£ –§–ê–ó–ê –û–ë–£–ß–ï–ù–ò–Ø")
    print("üí° –°–æ–≤–µ—Ç: –ü—Ä–µ—Ä–≤–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–Ω–æ —Å –ø–æ–º–æ—â—å—é Ctrl+C (–ø—Ä–æ–≥—Ä–µ—Å—Å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—Å—è)\n")

    # –ü–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
    agent.train(total_timesteps=50000)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 50k —à–∞–≥–æ–≤

    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    print("\n2Ô∏è‚É£ –§–ê–ó–ê –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    agent.test(episodes=5)

    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./models/improved_bot_latest.zip")
    print("üí° –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–Ω–æ–≤–∞ –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è –∏–ª–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")


if __name__ == "__main__":
    main()
