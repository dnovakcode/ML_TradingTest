#!/usr/bin/env python3
"""
üöÄ PRODUCTION-READY –¢–û–†–ì–û–í–´–ô –ë–û–¢ v7.0
–° —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–æ–º –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import os
from datetime import datetime
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

# ML imports
from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import BaseCallback
import torch
import torch.nn as nn

# –ù–∞—à–∏ –º–æ–¥—É–ª–∏
from data_fetcher import HistoricalDataFetcher
from risk_manager import DynamicRiskManager, RiskConfig
from metrics import TradingMetricsCalculator, PerformanceMetrics


@dataclass
class Trade:
    """–ö–ª–∞—Å—Å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–¥–µ–ª–∫–∏"""
    entry_price: float
    entry_time: int
    entry_index: int
    position_size: float
    trade_type: str
    stop_loss: float
    take_profit: float
    steps_held: int = 0


class ProductionTradingEnvironment(gym.Env):
    """
    Production-ready —Ç–æ—Ä–≥–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ —Å:
    - –†–µ–∞–ª—å–Ω—ã–º–∏ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    - –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç–æ–º
    - –£–ø—Ä–æ—â–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π –Ω–∞–≥—Ä–∞–¥ (—Ç–æ–ª—å–∫–æ PnL)
    - –î–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """

    def __init__(self, df: pd.DataFrame, initial_balance: float = 10000,
                 risk_config: Optional[RiskConfig] = None):
        super().__init__()

        self.df = df.reset_index(drop=True)
        self.initial_balance = initial_balance

        # –†–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç
        self.risk_manager = DynamicRiskManager(risk_config or RiskConfig())

        # Observation space: [—Ç–µ–∫—É—â–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã + —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è]
        # –†–∞–∑–º–µ—Ä –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–ª–æ–Ω–æ–∫ –≤ df
        n_features = len(df.columns) + 10  # +10 –¥–ª—è portfolio state
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(n_features,), dtype=np.float32
        )

        # Action space: –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ [-1, 1]
        # -1 = –ø—Ä–æ–¥–∞—Ç—å –≤—Å—ë, 0 = –¥–µ—Ä–∂–∞—Ç—å, +1 = –∫—É–ø–∏—Ç—å –º–∞–∫—Å–∏–º—É–º
        self.action_space = spaces.Box(
            low=np.array([-1.0], dtype=np.float32),
            high=np.array([1.0], dtype=np.float32),
            dtype=np.float32
        )

        self._init_state()

    def _init_state(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        self.balance = self.initial_balance
        self.btc_amount = 0.0
        self.current_step = 0

        # –¢–æ—Ä–≥–æ–≤–ª—è
        self.active_trade: Optional[Trade] = None
        self.closed_trades: List[Dict] = []

        # –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –º–µ—Ç—Ä–∏–∫
        self.portfolio_history = [self.initial_balance]
        self.balance_history = [self.initial_balance]

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_commission = 0.0

    def reset(self, seed: Optional[int] = None) -> Tuple[np.ndarray, Dict]:
        """–°–±—Ä–æ—Å –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
        super().reset(seed=seed)
        self._init_state()
        self.current_step = 0
        return self._get_observation(), {}

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —à–∞–≥–∞"""
        self.current_step += 1

        # –ó–∞—â–∏—Ç–∞ –æ—Ç –≤—ã—Ö–æ–¥–∞ –∑–∞ –≥—Ä–∞–Ω–∏—Ü—ã –¥–∞–Ω–Ω—ã—Ö
        if self.current_step >= len(self.df) - 1:
            return self._get_observation(), 0.0, True, False, self._get_info()

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        current_data = self.df.iloc[self.current_step]
        current_price = float(current_data['close'])

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –Ω–∞–≥—Ä–∞–¥—ã
        prev_portfolio = self._get_portfolio_value()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç–∏–≤–Ω—É—é —Å–¥–µ–ª–∫—É –Ω–∞ stop-loss/take-profit
        if self.active_trade:
            self.active_trade.steps_held += 1
            should_close, reason = self.risk_manager.should_close_position(
                entry_price=self.active_trade.entry_price,
                current_price=current_price,
                stop_loss=self.active_trade.stop_loss,
                take_profit=self.active_trade.take_profit,
                is_long=self.active_trade.trade_type == 'LONG',
                steps_held=self.active_trade.steps_held
            )

            if should_close:
                self._close_position(current_price, reason)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –¥–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞
        action_value = float(action[0])
        self._execute_action(action_value, current_price, current_data)

        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–∞–≥—Ä–∞–¥—É (–£–ü–†–û–©–ï–ù–ù–ê–Ø - —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è)
        current_portfolio = self._get_portfolio_value()
        reward = self._calculate_reward(prev_portfolio, current_portfolio)

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        self.portfolio_history.append(current_portfolio)
        self.balance_history.append(self.balance)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å–ª–æ–≤–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
        terminated = False
        truncated = current_portfolio < self.initial_balance * 0.5  # –ü–æ—Ç–µ—Ä—è 50%

        info = self._get_info()

        return self._get_observation(), reward, terminated, truncated, info

    def _execute_action(self, action: float, current_price: float, current_data: pd.Series):
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è"""

        # HOLD: -0.15 < action < 0.15
        if abs(action) < 0.15:
            return

        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é —ç–∫—Å–ø–æ–∑–∏—Ü–∏—é
        current_exposure = self.btc_amount * current_price

        # BUY: action >= 0.15
        if action >= 0.15 and not self.active_trade:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –æ—Ç–∫—Ä—ã—Ç–∏—è —Å–¥–µ–ª–∫–∏
            can_trade, reason = self.risk_manager.can_open_trade(
                balance=self.balance,
                current_exposure=current_exposure
            )

            if not can_trade:
                return

            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –ø–æ–∑–∏—Ü–∏–∏ —á–µ—Ä–µ–∑ —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç
            signal_strength = min(1.0, abs(action) * 1.5)
            volatility = float(current_data.get('volatility', 0.02))
            atr = float(current_data.get('atr', current_price * 0.02))

            position_size = self.risk_manager.calculate_position_size(
                balance=self.balance,
                current_price=current_price,
                signal_strength=signal_strength,
                volatility=volatility
            )

            if position_size < 100:
                return

            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–º–∏—Å—Å–∏—é
            commission = self.risk_manager.calculate_commission(position_size)

            if position_size + commission > self.balance:
                return

            # –û—Ç–∫—Ä—ã–≤–∞–µ–º —Å–¥–µ–ª–∫—É
            btc_to_buy = position_size / current_price
            self.balance -= (position_size + commission)
            self.btc_amount += btc_to_buy
            self.total_commission += commission

            # –í—ã—á–∏—Å–ª—è–µ–º stop-loss –∏ take-profit
            sl, tp = self.risk_manager.calculate_stop_loss_take_profit(
                entry_price=current_price,
                is_long=True,
                atr=atr
            )

            # –°–æ–∑–¥–∞–µ–º Trade –æ–±—ä–µ–∫—Ç
            self.active_trade = Trade(
                entry_price=current_price,
                entry_time=int(self.current_step),
                entry_index=self.current_step,
                position_size=btc_to_buy,
                trade_type='LONG',
                stop_loss=sl,
                take_profit=tp
            )

        # SELL: action <= -0.15
        elif action <= -0.15 and self.active_trade:
            self._close_position(current_price, "agent_decision")

    def _close_position(self, exit_price: float, reason: str):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–∑–∏—Ü–∏–∏"""
        if not self.active_trade:
            return

        # –í—ã—á–∏—Å–ª—è–µ–º –≤—ã—Ä—É—á–∫—É
        revenue = self.active_trade.position_size * exit_price
        commission = self.risk_manager.calculate_commission(revenue)

        # –û–±–Ω–æ–≤–ª—è–µ–º –±–∞–ª–∞–Ω—Å
        self.balance += (revenue - commission)
        self.btc_amount -= self.active_trade.position_size
        self.total_commission += commission

        # –í—ã—á–∏—Å–ª—è–µ–º PnL
        cost = self.active_trade.position_size * self.active_trade.entry_price
        pnl = revenue - cost
        pnl_after_commission = pnl - (commission * 2)  # –í—Ö–æ–¥ + –≤—ã—Ö–æ–¥
        pnl_pct = (pnl / cost) * 100

        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        trade_result = {
            'entry_price': self.active_trade.entry_price,
            'exit_price': exit_price,
            'entry_time': self.active_trade.entry_time,
            'exit_time': self.current_step,
            'duration': self.current_step - self.active_trade.entry_time,
            'steps_held': self.active_trade.steps_held,
            'pnl': pnl,
            'pnl_after_commission': pnl_after_commission,
            'pnl_percent': pnl_pct,
            'reason': reason
        }

        self.closed_trades.append(trade_result)

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–µ—Ä
        self.risk_manager.record_trade_result(pnl_after_commission)

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–¥–µ–ª–∫—É
        self.active_trade = None

    def _calculate_reward(self, prev_portfolio: float, current_portfolio: float) -> float:
        """
        –£–ü–†–û–©–ï–ù–ù–ê–Ø –ò –ß–ï–°–¢–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ù–ê–ì–†–ê–î
        –ù–∞–≥—Ä–∞–¥–∞ = —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö

        –≠—Ç–æ –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞:
        1. –ú–∞–∫—Å–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–±—ã–ª—å
        2. –ú–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —É–±—ã—Ç–∫–∏
        3. –ò–∑–±–µ–≥–∞—Ç—å –∫–æ–º–∏—Å—Å–∏–π (–¥–µ—Ä–∂–∞—Ç—å –ø–æ–∑–∏—Ü–∏–∏ –¥–æ–ª—å—à–µ)
        """
        portfolio_change = current_portfolio - prev_portfolio
        portfolio_change_pct = (portfolio_change / prev_portfolio) * 100

        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è: 1% = +100 reward
        reward = portfolio_change_pct * 100

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –Ω–∞–∫–∞–∑—ã–≤–∞–µ–º –∑–∞ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ —É–±—ã—Ç–∫–∏
        if portfolio_change_pct < -2:
            reward *= 1.5  # –£—Å–∏–ª–∏–≤–∞–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π —Å–∏–≥–Ω–∞–ª

        # Clip –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        reward = np.clip(reward, -500, 500)

        return float(reward)

    def _get_portfolio_value(self) -> float:
        """–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø–æ—Ä—Ç—Ñ–µ–ª—è"""
        current_price = float(self.df.iloc[self.current_step]['close'])
        return self.balance + (self.btc_amount * current_price)

    def _get_observation(self) -> np.ndarray:
        """–ü–æ–ª—É—á–∏—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ"""
        current_data = self.df.iloc[self.current_step]

        obs = []

        # 1. –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∏–∑ DataFrame
        close_price = float(current_data['close'])

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        for col in self.df.columns:
            if col in ['timestamp', 'open', 'high', 'low', 'volume']:
                continue

            value = float(current_data[col])

            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            if col == 'close':
                obs.append(value / 70000)  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–Ω—ã
            elif col.startswith('rsi'):
                obs.append(value / 100)  # RSI —É–∂–µ 0-100
            elif col.startswith('returns') or col.startswith('log_returns'):
                obs.append(value * 100)  # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            elif col.startswith('volume'):
                obs.append(np.log1p(value) / 20)  # Log –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—ä–µ–º–∞
            else:
                # –û–±—â–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –¥—Ä—É–≥–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
                obs.append(value / close_price if close_price > 0 else 0)

        # 2. –°–æ—Å—Ç–æ—è–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–µ–ª—è (10 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        portfolio_value = self._get_portfolio_value()

        obs.extend([
            self.balance / self.initial_balance,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –±–∞–ª–∞–Ω—Å
            (self.btc_amount * close_price) / self.initial_balance,  # –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–æ–∑–∏—Ü–∏—è
            (portfolio_value - self.initial_balance) / self.initial_balance,  # ROI
            1.0 if self.active_trade else 0.0,  # –ï—Å—Ç—å –ª–∏ –∞–∫—Ç–∏–≤–Ω–∞—è —Å–¥–µ–ª–∫–∞
        ])

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–∫—Ç–∏–≤–Ω–æ–π —Å–¥–µ–ª–∫–µ
        if self.active_trade:
            unrealized_pnl = (close_price - self.active_trade.entry_price) * self.active_trade.position_size
            unrealized_pnl_pct = ((close_price - self.active_trade.entry_price) /
                                 self.active_trade.entry_price) * 100

            obs.extend([
                unrealized_pnl / self.initial_balance,
                unrealized_pnl_pct / 10,  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                self.active_trade.steps_held / 100,
                (close_price - self.active_trade.stop_loss) / close_price,
                (self.active_trade.take_profit - close_price) / close_price,
            ])
        else:
            obs.extend([0, 0, 0, 0, 0])

        # –ù–µ–¥–∞–≤–Ω—è—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        if len(self.closed_trades) > 0:
            recent_trades = self.closed_trades[-5:]
            wins = sum(1 for t in recent_trades if t['pnl_after_commission'] > 0)
            win_rate = wins / len(recent_trades)
            obs.append(win_rate)
        else:
            obs.append(0.5)

        return np.array(obs, dtype=np.float32)

    def _get_info(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏"""
        return {
            'step': self.current_step,
            'portfolio_value': self._get_portfolio_value(),
            'balance': self.balance,
            'btc_amount': self.btc_amount,
            'active_trade': self.active_trade is not None,
            'closed_trades': len(self.closed_trades),
            'total_commission': self.total_commission,
            'risk_status': self.risk_manager.get_risk_status(self.balance)
        }


class DetailedCallback(BaseCallback):
    """Callback –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""

    def __init__(self, eval_env: ProductionTradingEnvironment,
                 log_freq: int = 1000, eval_freq: int = 5000):
        super().__init__()
        self.eval_env = eval_env
        self.log_freq = log_freq
        self.eval_freq = eval_freq
        self.best_sharpe = -np.inf
        self.metrics_calculator = TradingMetricsCalculator()

    def _on_step(self) -> bool:
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
        if self.n_calls % self.eval_freq == 0:
            self._evaluate_agent()

        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
        if self.n_calls % self.log_freq == 0:
            if 'infos' in self.locals and len(self.locals['infos']) > 0:
                info = self.locals['infos'][0]
                print(f"\nüìä –®–∞–≥ {self.n_calls}:")
                print(f"  Portfolio: ${info['portfolio_value']:.2f}")
                print(f"  –°–¥–µ–ª–æ–∫ –∑–∞–∫—Ä—ã—Ç–æ: {info['closed_trades']}")
                print(f"  –†–∏—Å–∫: {info['risk_status']['risk_level']}")

        return True

    def _evaluate_agent(self):
        """–û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞"""
        print(f"\n{'='*60}")
        print(f"üß™ –û–¶–ï–ù–ö–ê –ê–ì–ï–ù–¢–ê (—à–∞–≥ {self.n_calls})")
        print(f"{'='*60}")

        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ train environment
        if len(self.eval_env.portfolio_history) > 10:
            metrics = self.metrics_calculator.calculate_metrics(
                portfolio_values=self.eval_env.portfolio_history,
                closed_trades=self.eval_env.closed_trades,
                days_traded=len(self.eval_env.portfolio_history) / 24  # –ï—Å–ª–∏ 1h —Å–≤–µ—á–∏
            )

            print(f"üí∞ ROI: {metrics.total_return_pct:+.2f}%")
            print(f"üìà Sharpe: {metrics.sharpe_ratio:.3f}")
            print(f"üéØ Win Rate: {metrics.win_rate:.1f}%")
            print(f"üíé Profit Factor: {metrics.profit_factor:.2f}")
            print(f"‚ö†Ô∏è  Max DD: {metrics.max_drawdown_pct:.2f}%")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
            if metrics.sharpe_ratio > self.best_sharpe:
                self.best_sharpe = metrics.sharpe_ratio
                self.model.save("./models/best_model_by_sharpe")
                print(f"\nüåü –ù–û–í–´–ô –†–ï–ö–û–†–î Sharpe! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.")

        print(f"{'='*60}\n")


def train_agent(train_df: pd.DataFrame, val_df: pd.DataFrame,
                total_timesteps: int = 200000):
    """–û–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞"""

    print(f"""
{'='*80}
üöÄ –ó–ê–ü–£–°–ö –û–ë–£–ß–ï–ù–ò–Ø PRODUCTION-READY –ë–û–¢–ê
{'='*80}
üìä Train –¥–∞–Ω–Ω—ã—Ö: {len(train_df)} —Å–≤–µ—á–µ–π
üìä Val –¥–∞–Ω–Ω—ã—Ö:   {len(val_df)} —Å–≤–µ—á–µ–π
üéØ –®–∞–≥–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {total_timesteps:,}
{'='*80}
    """)

    # –°–æ–∑–¥–∞–µ–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ
    risk_config = RiskConfig(
        max_position_size_pct=15.0,
        max_risk_per_trade_pct=2.0,
        default_stop_loss_pct=3.0,
        default_take_profit_pct=6.0,
        max_drawdown_pct=20.0
    )

    train_env = ProductionTradingEnvironment(train_df, initial_balance=10000, risk_config=risk_config)

    # –°–æ–∑–¥–∞–µ–º –∞–≥–µ–Ω—Ç–∞
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üñ•Ô∏è  Device: {device}")

    policy_kwargs = dict(
        activation_fn=nn.ReLU,
        net_arch=dict(pi=[256, 256, 128], qf=[256, 256, 128]),
    )

    model = SAC(
        "MlpPolicy",
        train_env,
        learning_rate=3e-4,
        buffer_size=200000,
        learning_starts=5000,
        batch_size=256,
        tau=0.005,
        gamma=0.995,  # –í—ã—à–µ –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        ent_coef='auto',
        policy_kwargs=policy_kwargs,
        device=device,
        verbose=1
    )

    # Callback
    callback = DetailedCallback(train_env, log_freq=2000, eval_freq=10000)

    # –û–±—É—á–µ–Ω–∏–µ
    print("\nüéì –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø...\n")
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            progress_bar=True
        )
        print("\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –û–±—É—á–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ")

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    os.makedirs("./models", exist_ok=True)
    model.save("./models/trading_bot_pro_final")
    print("üíæ –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")

    return model, train_env


def evaluate_agent(model, test_df: pd.DataFrame, initial_balance: float = 10000):
    """–û—Ü–µ–Ω–∫–∞ –∞–≥–µ–Ω—Ç–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""

    print(f"""
{'='*80}
üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê OUT-OF-SAMPLE –î–ê–ù–ù–´–•
{'='*80}
üìä –¢–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {len(test_df)} —Å–≤–µ—á–µ–π
üí∞ –ù–∞—á–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å: ${initial_balance:,}
{'='*80}
    """)

    risk_config = RiskConfig()
    test_env = ProductionTradingEnvironment(test_df, initial_balance=initial_balance,
                                           risk_config=risk_config)

    obs, _ = test_env.reset()
    done = False
    truncated = False

    while not done and not truncated:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, truncated, info = test_env.step(action)

    # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìä –§–ò–ù–ê–õ–¨–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\n")

    calculator = TradingMetricsCalculator(initial_balance=initial_balance)
    metrics = calculator.calculate_metrics(
        portfolio_values=test_env.portfolio_history,
        closed_trades=test_env.closed_trades,
        days_traded=len(test_df) / 24  # –î–ª—è 1h —Å–≤–µ—á–µ–π
    )

    calculator.print_metrics(metrics)

    return metrics, test_env


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""

    print("""
    ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
    ‚ïë     üöÄ PRODUCTION-READY –¢–û–†–ì–û–í–´–ô –ë–û–¢ v7.0 üöÄ            ‚ïë
    ‚ïë                                                           ‚ïë
    ‚ïë  ‚úÖ –†–µ–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ                         ‚ïë
    ‚ïë  ‚úÖ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–∏—Å–∫-–º–µ–Ω–µ–¥–∂–º–µ–Ω—Ç                     ‚ïë
    ‚ïë  ‚úÖ Stop-loss & Take-profit                              ‚ïë
    ‚ïë  ‚úÖ –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π position sizing                         ‚ïë
    ‚ïë  ‚úÖ –ß–µ—Å—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –Ω–∞–≥—Ä–∞–¥ (—Ç–æ–ª—å–∫–æ PnL)                  ‚ïë
    ‚ïë  ‚úÖ –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (Sharpe, Sortino, etc.)    ‚ïë
    ‚ïë  ‚úÖ Walk-forward validation                              ‚ïë
    ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
    """)

    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    print("\nüì• –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•...")
    fetcher = HistoricalDataFetcher(symbol='BTC/USDT', timeframe='1h')
    df = fetcher.fetch_data(days=365, force_refresh=False)
    df = fetcher.add_technical_indicators(df)

    # 2. –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/val/test
    train_df, val_df, test_df = fetcher.split_data(df, train_ratio=0.7, val_ratio=0.15)

    # 3. –û–±—É—á–µ–Ω–∏–µ
    model, train_env = train_agent(train_df, val_df, total_timesteps=200000)

    # 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
    metrics, test_env = evaluate_agent(model, test_df)

    print("\n‚úÖ –ì–û–¢–û–í–û!")
    print("üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ ./models/trading_bot_pro_final.zip")
    print("üéØ –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–∏, –µ—Å–ª–∏ –º–µ—Ç—Ä–∏–∫–∏ —Ö–æ—Ä–æ—à–∏–µ!")


if __name__ == "__main__":
    main()
