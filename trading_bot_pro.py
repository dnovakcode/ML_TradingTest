"""
ğŸš€ PRODUCTION-READY Ğ¢ĞĞ Ğ“ĞĞ’Ğ«Ğ™ Ğ‘ĞĞ¢ v7.0
Ğ¡ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸, Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ¸ÑĞº-Ğ¼ĞµĞ½ĞµĞ´Ğ¶Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ¼ Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import os
from datetime import datetime
from typing import Dict, Tuple, Optional, List
from dataclasses import dataclass
import warnings
warnings.filterwarnings('ignore')

from stable_baselines3 import SAC
from stable_baselines3.common.callbacks import BaseCallback
import torch
import torch.nn as nn

from data_fetcher import HistoricalDataFetcher
from risk_manager import DynamicRiskManager, RiskConfig
from metrics import TradingMetricsCalculator, PerformanceMetrics


@dataclass
class Trade:
    """ĞšĞ»Ğ°ÑÑ Ğ´Ğ»Ñ Ğ¾Ñ‚ÑĞ»ĞµĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ ÑĞ´ĞµĞ»ĞºĞ¸"""
    entry_price: float
    entry_time: int
    entry_index: int
    position_size: float
    trade_type: str
    stop_loss: float
    take_profit: float
    steps_held: int = 0


class ProductionTradingEnvironment(gym.Env):
    """
    Production-ready Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ¾Ğµ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ñ:
    - Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼Ğ¸
    - ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ñ€Ğ¸ÑĞº-Ğ¼ĞµĞ½ĞµĞ´Ğ¶Ğ¼ĞµĞ½Ñ‚Ğ¾Ğ¼
    - Ğ£Ğ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ğ¾Ğ¹ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¾Ğ¹ Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ PnL)
    - Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ°Ğ¼Ğ¸
    """

    def __init__(self, df: pd.DataFrame, initial_balance: float = 10000,
                 risk_config: Optional[RiskConfig] = None):
        super().__init__()

        self.df = df.reset_index(drop=True)
        self.initial_balance = initial_balance

        self.risk_manager = DynamicRiskManager(risk_config or RiskConfig())

        n_features = len(df.columns) + 10  # +10 Ğ´Ğ»Ñ portfolio state
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(n_features,), dtype=np.float32
        )

        self.action_space = spaces.Box(
            low=np.array([-1.0], dtype=np.float32),
            high=np.array([1.0], dtype=np.float32),
            dtype=np.float32
        )

        self._init_state()

    def _init_state(self):
        """Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ñ"""
        self.balance = self.initial_balance
        self.btc_amount = 0.0
        self.current_step = 0

        self.active_trade: Optional[Trade] = None
        self.closed_trades: List[Dict] = []

        self.portfolio_history = [self.initial_balance]
        self.balance_history = [self.initial_balance]

        self.total_commission = 0.0

    def reset(self, seed: Optional[int] = None) -> Tuple[np.ndarray, Dict]:
        """Ğ¡Ğ±Ñ€Ğ¾Ñ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ"""
        super().reset(seed=seed)
        self._init_state()
        self.current_step = 0
        return self._get_observation(), {}

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, bool, Dict]:
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ ÑˆĞ°Ğ³Ğ°"""
        self.current_step += 1

        if self.current_step >= len(self.df) - 1:
            return self._get_observation(), 0.0, True, False, self._get_info()

        current_data = self.df.iloc[self.current_step]
        current_price = float(current_data['close'])

        prev_portfolio = self._get_portfolio_value()

        if self.active_trade:
            self.active_trade.steps_held += 1
            should_close, reason = self.risk_manager.should_close_position(
                entry_price=self.active_trade.entry_price,
                current_price=current_price,
                stop_loss=self.active_trade.stop_loss,
                take_profit=self.active_trade.take_profit,
                is_long=self.active_trade.trade_type == 'LONG',
                steps_held=self.active_trade.steps_held
            )

            if should_close:
                self._close_position(current_price, reason)

        action_value = float(action[0])
        self._execute_action(action_value, current_price, current_data)

        current_portfolio = self._get_portfolio_value()
        reward = self._calculate_reward(prev_portfolio, current_portfolio)

        self.portfolio_history.append(current_portfolio)
        self.balance_history.append(self.balance)

        terminated = False
        truncated = current_portfolio < self.initial_balance * 0.5  # ĞŸĞ¾Ñ‚ĞµÑ€Ñ 50%

        info = self._get_info()

        return self._get_observation(), reward, terminated, truncated, info

    def _execute_action(self, action: float, current_price: float, current_data: pd.Series):
        """Ğ’Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½Ğ¸Ğµ Ğ´ĞµĞ¹ÑÑ‚Ğ²Ğ¸Ñ"""

        if abs(action) < 0.15:
            return

        current_exposure = self.btc_amount * current_price

        if action >= 0.15 and not self.active_trade:
            can_trade, reason = self.risk_manager.can_open_trade(
                balance=self.balance,
                current_exposure=current_exposure
            )

            if not can_trade:
                return

            signal_strength = min(1.0, abs(action) * 1.5)
            volatility = float(current_data.get('volatility', 0.02))
            atr = float(current_data.get('atr', current_price * 0.02))

            position_size = self.risk_manager.calculate_position_size(
                balance=self.balance,
                current_price=current_price,
                signal_strength=signal_strength,
                volatility=volatility
            )

            if position_size < 100:
                return

            commission = self.risk_manager.calculate_commission(position_size)

            if position_size + commission > self.balance:
                return

            btc_to_buy = position_size / current_price
            self.balance -= (position_size + commission)
            self.btc_amount += btc_to_buy
            self.total_commission += commission

            sl, tp = self.risk_manager.calculate_stop_loss_take_profit(
                entry_price=current_price,
                is_long=True,
                atr=atr
            )

            self.active_trade = Trade(
                entry_price=current_price,
                entry_time=int(self.current_step),
                entry_index=self.current_step,
                position_size=btc_to_buy,
                trade_type='LONG',
                stop_loss=sl,
                take_profit=tp
            )

        elif action <= -0.15 and self.active_trade:
            self._close_position(current_price, "agent_decision")

    def _close_position(self, exit_price: float, reason: str):
        """Ğ—Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ğµ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸"""
        if not self.active_trade:
            return

        revenue = self.active_trade.position_size * exit_price
        commission = self.risk_manager.calculate_commission(revenue)

        self.balance += (revenue - commission)
        self.btc_amount -= self.active_trade.position_size
        self.total_commission += commission

        cost = self.active_trade.position_size * self.active_trade.entry_price
        pnl = revenue - cost
        pnl_after_commission = pnl - (commission * 2)  # Ğ’Ñ…Ğ¾Ğ´ + Ğ²Ñ‹Ñ…Ğ¾Ğ´
        pnl_pct = (pnl / cost) * 100

        trade_result = {
            'entry_price': self.active_trade.entry_price,
            'exit_price': exit_price,
            'entry_time': self.active_trade.entry_time,
            'exit_time': self.current_step,
            'duration': self.current_step - self.active_trade.entry_time,
            'steps_held': self.active_trade.steps_held,
            'pnl': pnl,
            'pnl_after_commission': pnl_after_commission,
            'pnl_percent': pnl_pct,
            'reason': reason
        }

        self.closed_trades.append(trade_result)

        self.risk_manager.record_trade_result(pnl_after_commission)

        self.active_trade = None

    def _calculate_reward(self, prev_portfolio: float, current_portfolio: float) -> float:
        """
        Ğ£ĞŸĞ ĞĞ©Ğ•ĞĞĞĞ¯ Ğ˜ Ğ§Ğ•Ğ¡Ğ¢ĞĞĞ¯ Ğ¡Ğ˜Ğ¡Ğ¢Ğ•ĞœĞ ĞĞĞ“Ğ ĞĞ”
        ĞĞ°Ğ³Ñ€Ğ°Ğ´Ğ° = Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾Ñ€Ñ‚Ñ„ĞµĞ»Ñ Ğ² Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ°Ñ…

        Ğ­Ñ‚Ğ¾ Ğ·Ğ°ÑÑ‚Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°:
        1. ĞœĞ°ĞºÑĞ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ
        2. ĞœĞ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑƒĞ±Ñ‹Ñ‚ĞºĞ¸
        3. Ğ˜Ğ·Ğ±ĞµĞ³Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ¼Ğ¸ÑÑĞ¸Ğ¹ (Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ÑŒ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ğ¸ Ğ´Ğ¾Ğ»ÑŒÑˆĞµ)
        """
        portfolio_change = current_portfolio - prev_portfolio
        portfolio_change_pct = (portfolio_change / prev_portfolio) * 100

        reward = portfolio_change_pct * 100

        if portfolio_change_pct < -2:
            reward *= 1.5  # Ğ£ÑĞ¸Ğ»Ğ¸Ğ²Ğ°ĞµĞ¼ Ğ½ĞµĞ³Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğ¹ ÑĞ¸Ğ³Ğ½Ğ°Ğ»

        reward = np.clip(reward, -500, 500)

        return float(reward)

    def _get_portfolio_value(self) -> float:
        """ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ¾Ğ¸Ğ¼Ğ¾ÑÑ‚ÑŒ Ğ¿Ğ¾Ñ€Ñ‚Ñ„ĞµĞ»Ñ"""
        current_price = float(self.df.iloc[self.current_step]['close'])
        return self.balance + (self.btc_amount * current_price)

    def _get_observation(self) -> np.ndarray:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ"""
        current_data = self.df.iloc[self.current_step]

        obs = []

        close_price = float(current_data['close'])

        for col in self.df.columns:
            if col in ['timestamp', 'open', 'high', 'low', 'volume']:
                continue

            value = float(current_data[col])

            if col == 'close':
                obs.append(value / 70000)  # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ñ†ĞµĞ½Ñ‹
            elif col.startswith('rsi'):
                obs.append(value / 100)  # RSI ÑƒĞ¶Ğµ 0-100
            elif col.startswith('returns') or col.startswith('log_returns'):
                obs.append(value * 100)  # ĞŸÑ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½Ğ¸Ñ
            elif col.startswith('volume'):
                obs.append(np.log1p(value) / 20)  # Log Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ±ÑŠĞµĞ¼Ğ°
            else:
                obs.append(value / close_price if close_price > 0 else 0)

        portfolio_value = self._get_portfolio_value()

        obs.extend([
            self.balance / self.initial_balance,  # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ
            (self.btc_amount * close_price) / self.initial_balance,  # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ°Ñ Ğ¿Ğ¾Ğ·Ğ¸Ñ†Ğ¸Ñ
            (portfolio_value - self.initial_balance) / self.initial_balance,  # ROI
            1.0 if self.active_trade else 0.0,  # Ğ•ÑÑ‚ÑŒ Ğ»Ğ¸ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ°Ñ ÑĞ´ĞµĞ»ĞºĞ°
        ])

        if self.active_trade:
            unrealized_pnl = (close_price - self.active_trade.entry_price) * self.active_trade.position_size
            unrealized_pnl_pct = ((close_price - self.active_trade.entry_price) /
                                 self.active_trade.entry_price) * 100

            obs.extend([
                unrealized_pnl / self.initial_balance,
                unrealized_pnl_pct / 10,  # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ
                self.active_trade.steps_held / 100,
                (close_price - self.active_trade.stop_loss) / close_price,
                (self.active_trade.take_profit - close_price) / close_price,
            ])
        else:
            obs.extend([0, 0, 0, 0, 0])

        if len(self.closed_trades) > 0:
            recent_trades = self.closed_trades[-5:]
            wins = sum(1 for t in recent_trades if t['pnl_after_commission'] > 0)
            win_rate = wins / len(recent_trades)
            obs.append(win_rate)
        else:
            obs.append(0.5)

        return np.array(obs, dtype=np.float32)

    def _get_info(self) -> Dict:
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ¾ÑÑ‚Ğ¾ÑĞ½Ğ¸Ğ¸"""
        return {
            'step': self.current_step,
            'portfolio_value': self._get_portfolio_value(),
            'balance': self.balance,
            'btc_amount': self.btc_amount,
            'active_trade': self.active_trade is not None,
            'closed_trades': len(self.closed_trades),
            'total_commission': self.total_commission,
            'risk_status': self.risk_manager.get_risk_status(self.balance)
        }


class DetailedCallback(BaseCallback):
    """Callback Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¼Ğ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³Ğ°"""

    def __init__(self, eval_env: ProductionTradingEnvironment,
                 log_freq: int = 1000, eval_freq: int = 5000):
        super().__init__()
        self.eval_env = eval_env
        self.log_freq = log_freq
        self.eval_freq = eval_freq
        self.best_sharpe = -np.inf
        self.metrics_calculator = TradingMetricsCalculator()

    def _on_step(self) -> bool:
        if self.n_calls % self.eval_freq == 0:
            self._evaluate_agent()

        if self.n_calls % self.log_freq == 0:
            if 'infos' in self.locals and len(self.locals['infos']) > 0:
                info = self.locals['infos'][0]
                print(f"\nğŸ“Š Ğ¨Ğ°Ğ³ {self.n_calls}:")
                print(f"  Portfolio: ${info['portfolio_value']:.2f}")
                print(f"  Ğ¡Ğ´ĞµĞ»Ğ¾Ğº Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¾: {info['closed_trades']}")
                print(f"  Ğ Ğ¸ÑĞº: {info['risk_status']['risk_level']}")

        return True

    def _evaluate_agent(self):
        """ĞÑ†ĞµĞ½ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""
        print(f"\n{'='*60}")
        print(f"ğŸ§ª ĞĞ¦Ğ•ĞĞšĞ ĞĞ“Ğ•ĞĞ¢Ğ (ÑˆĞ°Ğ³ {self.n_calls})")
        print(f"{'='*60}")

        if len(self.eval_env.portfolio_history) > 10:
            metrics = self.metrics_calculator.calculate_metrics(
                portfolio_values=self.eval_env.portfolio_history,
                closed_trades=self.eval_env.closed_trades,
                days_traded=len(self.eval_env.portfolio_history) / 24  # Ğ•ÑĞ»Ğ¸ 1h ÑĞ²ĞµÑ‡Ğ¸
            )

            print(f"ğŸ’° ROI: {metrics.total_return_pct:+.2f}%")
            print(f"ğŸ“ˆ Sharpe: {metrics.sharpe_ratio:.3f}")
            print(f"ğŸ¯ Win Rate: {metrics.win_rate:.1f}%")
            print(f"ğŸ’ Profit Factor: {metrics.profit_factor:.2f}")
            print(f"âš ï¸  Max DD: {metrics.max_drawdown_pct:.2f}%")

            if metrics.sharpe_ratio > self.best_sharpe:
                self.best_sharpe = metrics.sharpe_ratio
                self.model.save("./models/best_model_by_sharpe")
                print(f"\nğŸŒŸ ĞĞĞ’Ğ«Ğ™ Ğ Ğ•ĞšĞĞ Ğ” Sharpe! ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°.")

        print(f"{'='*60}\n")


def train_agent(train_df: pd.DataFrame, val_df: pd.DataFrame,
                total_timesteps: int = 200000):
    """ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ°Ğ³ĞµĞ½Ñ‚Ğ°"""

    print(f"""
{'='*80}
ğŸš€ Ğ—ĞĞŸĞ£Ğ¡Ğš ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯ PRODUCTION-READY Ğ‘ĞĞ¢Ğ
{'='*80}
ğŸ“Š Train Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {len(train_df)} ÑĞ²ĞµÑ‡ĞµĞ¹
ğŸ“Š Val Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…:   {len(val_df)} ÑĞ²ĞµÑ‡ĞµĞ¹
ğŸ¯ Ğ¨Ğ°Ğ³Ğ¾Ğ² Ğ¾Ğ±ÑƒÑ‡ĞµĞ½Ğ¸Ñ: {total_timesteps:,}
{'='*80}
    """)

    risk_config = RiskConfig(
        max_position_size_pct=15.0,
        max_risk_per_trade_pct=2.0,
        default_stop_loss_pct=3.0,
        default_take_profit_pct=6.0,
        max_drawdown_pct=20.0
    )

    train_env = ProductionTradingEnvironment(train_df, initial_balance=10000, risk_config=risk_config)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ–¥ï¸  Device: {device}")

    policy_kwargs = dict(
        activation_fn=nn.ReLU,
        net_arch=dict(pi=[256, 256, 128], qf=[256, 256, 128]),
    )

    model = SAC(
        "MlpPolicy",
        train_env,
        learning_rate=3e-4,
        buffer_size=200000,
        learning_starts=5000,
        batch_size=256,
        tau=0.005,
        gamma=0.995,  # Ğ’Ñ‹ÑˆĞµ Ğ´Ğ»Ñ Ğ´Ğ¾Ğ»Ğ³Ğ¾ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ñ… ÑÑ‚Ñ€Ğ°Ñ‚ĞµĞ³Ğ¸Ğ¹
        ent_coef='auto',
        policy_kwargs=policy_kwargs,
        device=device,
        verbose=1
    )

    callback = DetailedCallback(train_env, log_freq=2000, eval_freq=10000)

    print("\nğŸ“ ĞĞĞ§ĞĞ›Ğ ĞĞ‘Ğ£Ğ§Ğ•ĞĞ˜Ğ¯...\n")
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            progress_bar=True
        )
        print("\nâœ… ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ¾!")
    except KeyboardInterrupt:
        print("\nâš ï¸ ĞĞ±ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€ĞµÑ€Ğ²Ğ°Ğ½Ğ¾")

    os.makedirs("./models", exist_ok=True)
    model.save("./models/trading_bot_pro_final")
    print("ğŸ’¾ Ğ¤Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ°")

    return model, train_env


def evaluate_agent(model, test_df: pd.DataFrame, initial_balance: float = 10000):
    """ĞÑ†ĞµĞ½ĞºĞ° Ğ°Ğ³ĞµĞ½Ñ‚Ğ° Ğ½Ğ° Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…"""

    print(f"""
{'='*80}
ğŸ§ª Ğ¢Ğ•Ğ¡Ğ¢Ğ˜Ğ ĞĞ’ĞĞĞ˜Ğ• ĞĞ OUT-OF-SAMPLE Ğ”ĞĞĞĞ«Ğ¥
{'='*80}
ğŸ“Š Ğ¢ĞµÑÑ‚Ğ¾Ğ²Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…: {len(test_df)} ÑĞ²ĞµÑ‡ĞµĞ¹
ğŸ’° ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ»Ğ°Ğ½Ñ: ${initial_balance:,}
{'='*80}
    """)

    risk_config = RiskConfig()
    test_env = ProductionTradingEnvironment(test_df, initial_balance=initial_balance,
                                           risk_config=risk_config)

    obs, _ = test_env.reset()
    done = False
    truncated = False

    while not done and not truncated:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, done, truncated, info = test_env.step(action)

    print("\nğŸ“Š Ğ¤Ğ˜ĞĞĞ›Ğ¬ĞĞ«Ğ• Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ«:\n")

    calculator = TradingMetricsCalculator(initial_balance=initial_balance)
    metrics = calculator.calculate_metrics(
        portfolio_values=test_env.portfolio_history,
        closed_trades=test_env.closed_trades,
        days_traded=len(test_df) / 24  # Ğ”Ğ»Ñ 1h ÑĞ²ĞµÑ‡ĞµĞ¹
    )

    calculator.print_metrics(metrics)

    return metrics, test_env


def main():
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ğ°Ñ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ"""

    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘     ğŸš€ PRODUCTION-READY Ğ¢ĞĞ Ğ“ĞĞ’Ğ«Ğ™ Ğ‘ĞĞ¢ v7.0 ğŸš€            â•‘
    â•‘                                                           â•‘
    â•‘  âœ… Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ                         â•‘
    â•‘  âœ… ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ¸ÑĞº-Ğ¼ĞµĞ½ĞµĞ´Ğ¶Ğ¼ĞµĞ½Ñ‚                     â•‘
    â•‘  âœ… Stop-loss & Take-profit                              â•‘
    â•‘  âœ… Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ position sizing                         â•‘
    â•‘  âœ… Ğ§ĞµÑÑ‚Ğ½Ğ°Ñ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğ°Ğ³Ñ€Ğ°Ğ´ (Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ PnL)                  â•‘
    â•‘  âœ… ĞŸÑ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ (Sharpe, Sortino, etc.)    â•‘
    â•‘  âœ… Walk-forward validation                              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    print("\nğŸ“¥ Ğ—ĞĞ“Ğ Ğ£Ğ—ĞšĞ Ğ”ĞĞĞĞ«Ğ¥...")
    fetcher = HistoricalDataFetcher(symbol='BTC/USDT', timeframe='1h')
    df = fetcher.fetch_data(days=365, force_refresh=False)
    df = fetcher.add_technical_indicators(df)

    train_df, val_df, test_df = fetcher.split_data(df, train_ratio=0.7, val_ratio=0.15)

    model, train_env = train_agent(train_df, val_df, total_timesteps=200000)

    metrics, test_env = evaluate_agent(model, test_df)

    print("\nâœ… Ğ“ĞĞ¢ĞĞ’Ğ!")
    print("ğŸ’¾ ĞœĞ¾Ğ´ĞµĞ»ÑŒ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ° Ğ² ./models/trading_bot_pro_final.zip")
    print("ğŸ¯ ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ Ğ´Ğ»Ñ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ğ¸, ĞµÑĞ»Ğ¸ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ñ…Ğ¾Ñ€Ğ¾ÑˆĞ¸Ğµ!")


if __name__ == "__main__":
    main()
